---
layout: post
title: deep-learning 기본
date: 2021-05-03 19:00:00
---

실습은 Colab에서 진행

{% highlight python %}

%tensorflow_version 1.x

import tensorflow as tf

a = tf.constant(3.0)
b = tf.constant(4.0)
c = tf.constant(5.0)

d = a*b+c

# 17이 튀어나오지 않음
print(d)
{% endhighlight %}
> Tensor("add:0", shape=(), dtype=float32)

d라는 노드 자체를 출력. 노드의 마지막이 덧셈이므로 add가 튀어나옴. 

shape이 빈 괄호라는 것은 값이 scalar. 연산 후 숫자값만 나오기 때문.

데이터 타입은 float32

{% highlight python %}

sess = tf.Session()
# 실행할 세션을 만들고 실행을 해야 결과값을 볼 수 있음

result = sess.run(d)
print(result)

{% endhighlight %}
> 17.0

텐서플로의 큰 단점이 위의 사항

그래프를 다 만들어놓고 실행해야 한다는 static graph라는 단점이 있음

그런데 pytorch는 다이내믹 graph이기 때문에 텐서플로와 차이가 있음

**텐서플로우 2에서는 Dynamic graph로 동작함**

{% highlight python %}

a = tf.constant(3.0)
b = tf.constant(4.0)
c = tf.constant(5.0)

d = a*b+c

# tensorflow 2는 바로 값이 나옴
print(d)

{% endhighlight %}

### Tensorflow 1.x로 실습

{% highlight python %}

W = tf.Variable(tf.random_normal(shape=[1], name='w'))
b = tf.Variable(tf.random_normal(shape=[1], name='b'))
x = tf.placeholder(tf.float32) # 입력받을 값이니까 placeholder

pred = W*x + b

y = tf.placeholder(tf.float32)

loss = tf.reduce_mean(tf.square(pred - y))

optimizer = tf.train.GradientDescentOptimizer(0.01)

train_step = optimizer.minimize(loss)

# target model
# y = 2x + 1
# w = 2, b = 1 이 되어야 함. 맨 첨엔 랜덤. 학습으로 저렇게 줄이는 거.

x_train = [1,2,3,4,5,6]
y_train = [3,5,7,9,11,13]

sess = tf.Session()
result = sess.run(tf.global_variables_initializer())

for i in range(1000):
  sess.run(train_step, feed_dict = {x: x_train, y: y_train})

x_test = [1,2,3,7,8,9]
print(sess.run(pred, feed_dict= {x:x_test}))

print(sess.run(W))
# W = 2 타겟팅 맞췃고

print(sess.run(b))
# b = 1 타겟팅 맞췄다.


{% endhighlight %}
